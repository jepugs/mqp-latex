\subsection*{Definition of DSD}

\noindent In their original definition, Cao et al. define DSD in terms of a
vector-valued function $\mathrm{He}^{k}(u)$, which computes the expected number of times
a length-$k$ random walk originating at a vertex $u$ will visit each other vertex
in the graph. They compute DSD by taking the $l_1$-norm of the difference
between two such vectors in the limit as $k \to \infty$. We present an
alternative definition which is more compact and easier to manipulate with the
usual machinery of linear algebra.

Random walks can be modeled as Markov chains, where the current state is a
vertex $u$ in the graph, and the transition probability is $\frac{1}{\deg(u)}$
for the neighbors of $u$ and $0$ otherwise. The transition matrix can be written
$T = AD$, where $A$ is the adjacency matrix of the graph and $D$ is the unique
diagonal matrix such that every column of $T$ sums to 1 (i.e.
$D_{u,u}=\frac{1}{deg(u)}$, or equivalently, $D$ is the inverse of the degree
matrix). For our purposes, we assume that vertices are labeled by the positive
integers $1,2,...,n$ and that the rows and columns of the $G$ correspond to this
ordering.

The marginal distribution of vertices in the $k\nth$ step of a random walk
originating from vertex $u$ is given by $T^ke_u$, where $e_u$ is the initial
state vector, which is the $u\nth$ standard basis vector in $\R^n$,
$e_{u,u} := 1$ and $e_{u,j} := 0, u \neq j$ for $u=1,...,n$.

Thus, we can define the DSD, $\delta(u,v)$, by

\[
  \delta(u,v) = ||\sum_{k =0}^{\infty}{T^ke_u - T^ke_v}||_1
\]

\subsection{Proof that DSD is a Metric}

A metric is a nonnegative real-valued function that generalizes Euclidean
distance between points in $R^n$ to general sets. In our case, the set is the
vertices of the graph. Metrics allow us to define metric spaces which are
generally useful, but the context of our problem, we are mainly interested in
them because they allow us to partially order all vertices in a graph based on
their distance from a fixed vertex.

\begin{definition}
  A \textbf{metric} with respect to a set $S$ is a function $d : S x S \to \R$
  satisfying

  \begin{enumerate}
  \item $d(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $d(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $d(u,v) = \delta(v,u) ~\forall u,v \in G$, and
  \item $d(u,v) \leq \delta(u,w) + \delta(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}\end{definition}

First, we will define a finite analog to the $\mathrm{He(u)}$ vectors of Cao et al. Because
$T$ is a transition matrix for a Markov chain, this means that
$\lim_{k \to \infty} T^k$ converges to a stationary distribution which we will
call $T^\infty$.

\begin{lemma}
  Let $T$ be a transition matrix for a markov chain and $T^\infty$ the
  corresponding stationary distribution. The matrix sum

  \[
    S = \sum_{k=0}^{\infty}(T^k - T^\infty)
  \]

  converges.
\end{lemma}
\begin{proof}
  \textbf{TODO} [I thought this was an easy proof, but now I'm not sure -- JP]
\end{proof}

\begin{remark}
  $S$ is symmetric.
\end{remark}
\begin{proof}
  Clearly, since $T$ is symmetric and commutes with itself, $T^k$ is symmetric
  for all $k$. This implies that $T^\infty$ is also symmetric, as is the
  difference $T^k - T^\infty$ for all $k$. Thus, every term in the sum is
  symmetric, completing the proof.
\end{proof}

We will denote row vectors of matrices with a single subscript, i.e. $S_u$ is
the $u\nth$ row of $S$ which is equal to the $u\nth$ column. The columns of $S$
are our finite analog for the $\mathrm{He}$ vectors. Now, note that $T^ke_u$ is
equal to $T^k_u$. This allows us to rewrite DSD as

\begin{align*}
  \delta(u,v) &= ||\sum_{k=0}^{\infty}((T^k)_u - (T^k)_v)||_1 \\
              &= ||\sum_{k=0}^{\infty}((T^k-T^\infty)_u - (T^k-T^\infty)_v)||_1 \\
              &= ||S_u - S_v||_1
\end{align*}

\begin{theorem}
  DSD is a metric.
\end{theorem}
\begin{proof}
  We must show:

  \begin{enumerate}
  \item $\delta(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $\delta(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $\delta(u,v) = \delta(v,u) ~\forall u,v \in G$, and
  \item $\delta(u,v) \leq \delta(u,w) + \delta(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}

  Let $G = (V,E)$ be an arbitrary graph of order $n$ and define $T$, $T^\infty$,
  and $S$ as above. We define a map $f : V \to \R^n$ by $f(u) = S_u$. DSD is
  equivalent to taking the $l_1$-norm of differences between these points. Since
  $l_1$ distance is a metric in $\R^n$, we have shown (1), (3), and (4).

  Because $T$ is invertible, no two rows or columns of $T$ are equal, which also
  applies to the powers of $T$. Consequently, $S$ is also invertible. Thus,
  $f$ is injective, proving (2).

  % The eigenvectors of $T$ span $\R^n$, which also means that no two
  % rows of $T$ are equal, as otherwise $\dim T$ would be less than $n$. We
  % rewrite every row/column $T_u$ as a linear combination
  % \[
  %   T_u = \sum_{i=1}^n \theta_i(u) x_i
  % \]

  % for some coefficient vector $\theta(u)$, where $x_i$ are eigenvectors of $T$.
  % Let $\lambda_i$ be the respective eigenvalues. Because all the rows in $T$ are
  % distinct, every coefficient vector $\theta(u)$ is distinct. Also, the
  % corresponding coefficients for powers of $T$ are
  % $T^k_u = \sum{\lambda_i^{k-1} \theta_i(u) x_i}$.

  % Let $k$ be arbitrary, and suppose $T^k_u = T^k_v$ for some $u \neq v$. Then
  % $\lambda_i^{k-1}\theta_i(u) = \lambda_i^{k-1}\theta_i(v)$ for all
  % $i = 1,...,n$. Clearly, this only holds for the case that
  % $\theta_i(u) = \theta_i(v)$ for all $i$, which is a contradiction because it
  % implies $T_v = T_u$. Thus, every row (and column) of $T^k$ is distinct for all
  % powers $k$, and this property extends to $S$, thus proving (2).

\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
