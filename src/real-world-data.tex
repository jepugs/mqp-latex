In this section, we discuss three different examples of graph-based studies performed on real world data. We discuss how these examples constructed graphs from their data, and what kind of studies were performed on the resulting graphs.

\subsection{PPI networks}
~\cite{10.1371/journal.pone.0076339} studied the prediction of protein functional labels on the \emph{S. cerevisiae} protein-protein interaction network (PPI network). A graph was constructed using annotated physical interactions in this PPI network. Proteins corresponded to vertices, and an annotated physical interaction between two proteins corresponded to an edge. Redundant edges were removed and the largest connected component was selected, resulting in a graph of 4990 vertices and 74,310 edges. It was also noted that PPI networks are known to resemble small-world networks, as they have a small maximum shortest path, and a small characteristic path length. This implies that most vertices in the graph are close to every other vertex in the graph.

~\cite{10.1371/journal.pone.0076339} used four different prediction algorithms to compare the effectiveness of the DSD metric to that of the shortest path metric. Both weighted and unweighted majority voting algorithms, as well as the $\chi^{2}$ neighborhood algorithm, multi-way cut algorithm, and functional flow algorithm were used to compare the predictive performance of these metrics. The DSD metric was expected to perform better that the shortest path metric, since the small characteristic path length of the PPI network causes the notion of a neighborhood, with respect to the shortest path metric, to lose significance. All vertices in this network are close to every other vertex in the graph, so any neighborhood would contain a large portion of the vertices in the graph. The results of their experiments proved their hypotheses.


\subsection{Graph model for recommender systems}
~\cite{huang2004graph} introduces a generic graph model for e-commerce transaction data that can support various recommendation methods. The two-layer graph model proposed represents relationships between products and customers. In this model, each layer consists of vertices representing products or customers. Three types of edges in this two layer system capture the inputs to this model from real world data. Edges between two customers capture similarity based on available demographic data, answers to questionnaires, and web usage patterns. Edges between two products capture similarity using descriptions of the product. Finally, edges between customers and products capture transaction information such as purchase history, customer rating, and related browsing behavior.

~\cite{huang2004graph} considered three different recommendation methods to use on the model mentioned previously. The direct retrieval method looks at a customer's previous purchases and products purchased by similar customers to retrieve products to recommend to the customer. The association mining method uses the purchase history of a customer to generate association rules about purchasing patterns in order to predict the customer's next purchases. The high-degree association retrieval method uses weights on edges to represent association strength and examines paths between the target customer vertex and a product vertex to calculate an association estimate and determine if the product should be recommended. Transactions from one of the largest online bookstores in Taiwan were used for data including 9695 books, 2000 customers, and 18,771 transaction records.


\subsection{Laplacian Eigenmaps}
~\cite{belkin2002laplacian} proposes an algorithm for constructing a locality-preserving representation of data sampled from a low dimensional manifold embedded in a higher dimensional space. An example can be found in $n\times n$ gray scale images of a fixed object taken by a moving camera. These images give data points in $\mathbb{R}^{n^{2}}$, but the inherent dimensionality of this data should be the number of degrees of freedom of the camera. The presented algorithm constructs a representation of the data that reduces the dimensionality of the data.

~\cite{belkin2002laplacian} constructs a weighted graph from a given set of points in $\mathbb{R}^{n}$. Each point is represented by a vertex, and edges are drawn between neighboring points. Neighborhoods can be determined using $k$ nearest neighbors or $\epsilon$-neighborhoods. Weights are determined using heat kernels, or simply adjacency. Finally, eigenmaps are used to reduce dimensionality of the data.

$1000$ binary $40 \times 40$ images of vertical and horizontal bars at arbitrary points were randomly chosen as data. The Laplacian Eigenmap algorithm and Principal Component Analysis (PCA) was applied to this data and compared. The Laplacian Eigenmap algorithm produced a data representation that clearly showed separate clusters for the vertical and horizontal bars, which PCA failed to do.
