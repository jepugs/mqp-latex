In this chapter, we provide background about metrics on graph vertices, with a particular focus on
the diffusion state distance (DSD). The three prediction algorithms compared in Chapters
\ref{chap:simulations} and \ref{chap:real_world} are identical except for the choice of metric, as a
key part of our project is understanding how that choice affects label prediction performance. For
details about how our classification algorithm uses metrics to pick labels, see Chapter
\ref{chap:prediction}.


\section{Definition and Shortest-Path Distance}

A metric is a nonnegative real-valued function that generalizes Euclidean distance between points in
$\mathbb{R}^n$ to general sets. In our case, the set is the vertices of the graph, and in the context of the
label prediction problem, we use metrics to determine similiarity of two vertices in the graph.

\begin{definition}
  A \textbf{metric} on a set $S$ is a function $f : S \times S \to \R$
  satisfying

  \begin{enumerate}
  \item $f(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $f(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $f(u,v) = f(v,u) ~\forall u,v \in G$, and
  \item $f(u,v) \leq f(u,w) + f(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}
\end{definition}

The standard metric on graphs is the shortest-path distance (definition \ref{def:walk_path}), whose
definition we reiterate here for convenience. The \textbf{shortest-path distance} between two
vertices $u,v \in G$, denoted $\spd(u,v)$, is the minimum number of edges in a path from $u$ to $v$.

\begin{proposition}
  Shortest-path distance is a metric (on the vertices of connected graphs).
\end{proposition}

\begin{proof}
  From the definition of shortest-path distance, it is clear that metric conditions (1), (2),
  and (3) are satisfied, since we're dealing with unweighted, undirected graphs.

  To prove (4), let any three vertices in a connected graph $u,v,w$ be given. Define $a=\spd(u,w)$
  and $b=\spd(v,w)$. Then there exists a path from $u$ to $w$ of length $a$ and from $w$ to $v$ of
  length $b$, and concatenating these paths gives a path from $u$ to $v$ of length $a+b$. Thus, by
  definition of shortest-path distance, $\spd(u,v) \leq a + b = \spd(u,w) + \spd(v,w)$ for any three
  vertices $u,v,w$, which is more than sufficient to show (4).
\end{proof}


\section{Diffusion State Distance}

The diffusion state distance (DSD) is a metric on graphs which determines distance between two
vertices based upon the convergence behavior of random walks starting from each vertex. Intuitively,
it provides a way to measure vertex distance that is sensitive to the local structure of the graph,
and appears to be well suited to detecting community structures in graphs.

In the original paper, Cao, Zhang, Park, Daniels, Crovella, Cowen, and Hescott define DSD in terms
of a vector-valued function $\mathrm{He}^{k}(u)$, which computes the expected number of times a
length-$k$ random walk originating at a vertex $u$ will visit each other vertex in the graph. They
compute DSD by taking the $l_1$-norm of the difference between two such vectors in the limit as
$k \to \infty$. We present an alternative definition which is easier to manipulate with the usual
machinery of linear algebra. We begin by recalling some definitions.

\begin{definition}
  A \textbf{random walk} of a graph is a walk (see definition \ref{def:walk_path}) originating at
  some vertex in which each edge is picked uniformly randomly from all the edges adjoined to the
  vertex preceding it.
\end{definition}

%% TODO: find cite Bollobas
\begin{definition}[Page 302 of \cite{} ]
  A (discrete-time) \textbf{markov chain} on a finite set of states $V$ is a sequence of random
  variables $X_0,X_1,....$ taking values $x_o,x_1,... \in V$ such that for a given $t$, the
  probability of each outcome of $X_{t+1}$ depends only on $x_t$ (the outcome before it).

  Let $u,v \in V$. Then the conditional probability $P(X_{t+1}=v|X_t=u)$ is called the
  \textbf{transition probability} from $u$ to $v$.
  
  The \textbf{transition matrix} of a markov chain is the matrix $T$ given by

  \[
    T_{ij} = \text{The probability of transitioning from state $j$ to state $i$}
  \]
\end{definition}

Random walks may be modeled as markov chains where the states are the vertices in the graph. The
random walk transition matrix of a graph $G$ can be written $T = AD^{-1}$, where $A$ is the
adjacency and $D$ is the degree matrix, (definitions \ref{def:adj_mat} and \ref{def:deg_mat}). For
our purposes, we assume that vertices are labeled by the positive integers $1,2,...,n$ and that the
rows and columns of $G$ correspond to this ordering.

\begin{proposition}[Properties of $T$]
  The following is true:
  \begin{enumerate}
  \item $T$ is irreducible
  \item every eigenvalue of $T$ has magnitude $\leq 1$
  \end{enumerate}
\end{proposition}

\begin{proof}
  To show (1), we first recall from proposition \ref{prop:adj} that the adjacency matrix is
  irreducible. From the definition $T = AD^{-1}$, we can see that $T$ has zeros in the same set of
  indices as $A$, and so must be irreducible by analogous argument.

  (2) follows from (1) and application of the Perron-Frobenius Theorem (theorem
  \ref{thm:perron_frobenius}).
\end{proof}

Based on how we've defined that transition matrix, it can be thought of as an operator which takes a
marginal distribution of states in the markov chain, and outputs the marginal distribution after one
step. So, the marginal distribution of vertices in the $k\nth$ step of a random walk originating
from vertex $u$ is given by $T^ke_u$, where $e_u$ is the $u\nth$ standard basis vector in $\R^n$,
$e_{u,u} := 1$ and $e_{u,j} := 0, u \neq j$ for $u=1,...,n$.

Finally, we can define DSD as such

\begin{definition}
  Let $G$ be a graph, $u,v \in G$. The \textbf{diffusion state distance (DSD)}, denoted $\dsd(u,v)$,
  is defined by
  \[
    \dsd(u,v) = ||\sum_{k =0}^{\infty}{T^ke_u - T^ke_v}||_1
  \]
  where $T$ is the transition matrix of $G$'s random walk markov chain and $e_i$ is th $i\nth$
  standard basis vector in $\mathbb{R}^n$.
\end{definition}


\subsection{Proof that DSD is a Metric}

Here we prove that DSD is a metric. In order to present significantly more elegant proof, we will
restrict to the case where the graph $G$ is not bipartite. This is due to the following fact:

%% cite bollobas
\begin{theorem}[Page 310 of ]
  Let $G$ be a connected graph. The markov chain modeling random walks of $G$ converges to a
  stationary distribution if and only if $G$ is not bipartite.
\end{theorem}

\begin{definition}
  A markov chain is said to \textbf{converge to a stationary distribution} if the limit
  $\lim_{t \to \infty}T^t$ exists, where $T$ is the markov chain's transition matrix. In this case,
  we denote the stationary distribution by $T^\infty = \lim_{t \to \infty}T^t$
\end{definition}

For the rest of this subsection, we will let an arbitrary non-bipartite graph $G$ be given and let
$T$ and $T^{\infty}$ be defined as above.


\begin{proposition}
  Let $x$ be an eigenvector of $T$ with corresponding eigenvalue $\lambda$. Then $x$ is also
  an eigenvector of $T^{\infty}$, where it has corresponding eigenvalue
  \[
    \lambda^{\infty} = \begin{cases}
      0 &~: |\lambda| < 1 \\
      1 &~: |\lambda| = 1
    \end{cases}
  \]
\end{proposition}

\begin{proof}
  Let $\lambda$, $x$ be given such that $Tx = \lambda x$. Then

  \begin{align*}
    T^{\infty}x &= \lim_{k\to\infty}T^k x \\
               &= \lim_{k\to\infty}\lambda^k x \\
  \end{align*}

  \[ T^{\infty}x = \begin{cases}
      0 &~: |\lambda| < 1 \\
      x &~: |\lambda| = 1
    \end{cases}
  \]
\end{proof}

\begin{corollary}
  From the previous proof, it becomes clear that $-1$ is not an eigenvalue of $T$, as the existence
  of a corresponding eigenvector would create a contradiction on the existence of $T^{\infty}$.
\end{corollary}


We will denote row vectors of matrices with a single subscript, i.e. $S_u$ is
the $u\nth$ row of $S$ which is equal to the $u\nth$ column. The columns of $S$
are our finite analog for the $\mathrm{He}$ vectors. Now, note that $T^ke_u$ is
equal to $T^k_u$. This allows us to rewrite DSD as

\begin{align*}
  \dsd(u,v) &= ||\sum_{k=0}^{\infty}((T^k)_u - (T^k)_v)||_1 \\
              &= ||\sum_{k=0}^{\infty}((T^k-T^\infty)_u - (T^k-T^\infty)_v)||_1 \\
              &= ||S_u - S_v||_1
\end{align*}

\begin{theorem}
  DSD is a metric.
\end{theorem}
\begin{proof}
  We must show:

  \begin{enumerate}
  \item $\dsd(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $\dsd(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $\dsd(u,v) = \dsd(v,u) ~\forall u,v \in G$, and
  \item $\dsd(u,v) \leq \dsd(u,w) + \dsd(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}

  Let $G = (V,E)$ be an arbitrary graph of order $n$ and define $T$, $T^\infty$,
  and $S$ as above. We define a map $f : V \to \R^n$ by $f(u) = S_u$. DSD is
  equivalent to taking the $l_1$-norm of differences between these points. Since
  $l_1$ distance is a metric in $\R^n$, we have shown (1), (3), and (4).

  Because $T$ is invertible, no two rows or columns of $T$ are equal, which also
  applies to the powers of $T$. Consequently, $S$ is also invertible. Thus,
  $f$ is injective, proving (2).

  % The eigenvectors of $T$ span $\R^n$, which also means that no two
  % rows of $T$ are equal, as otherwise $\dim T$ would be less than $n$. We
  % rewrite every row/column $T_u$ as a linear combination
  % \[
  %   T_u = \sum_{i=1}^n \theta_i(u) x_i
  % \]

  % for some coefficient vector $\theta(u)$, where $x_i$ are eigenvectors of $T$.
  % Let $\lambda_i$ be the respective eigenvalues. Because all the rows in $T$ are
  % distinct, every coefficient vector $\theta(u)$ is distinct. Also, the
  % corresponding coefficients for powers of $T$ are
  % $T^k_u = \sum{\lambda_i^{k-1} \theta_i(u) x_i}$.

  % Let $k$ be arbitrary, and suppose $T^k_u = T^k_v$ for some $u \neq v$. Then
  % $\lambda_i^{k-1}\theta_i(u) = \lambda_i^{k-1}\theta_i(v)$ for all
  % $i = 1,...,n$. Clearly, this only holds for the case that
  % $\theta_i(u) = \theta_i(v)$ for all $i$, which is a contradiction because it
  % implies $T_v = T_u$. Thus, every row (and column) of $T^k$ is distinct for all
  % powers $k$, and this property extends to $S$, thus proving (2).

\end{proof}


\section{DSD on Special Graphs}

\subsection{Complete Graphs}
Consider the complete graph $K_n$. In this case, $D = \frac{1}{n-1}I$ and
$A = (J - I)$, so $T=\frac{1}{n-1}(J-I)$. In order to compute $T^ke_u$ in the
limit, we will represent $e_u$ as a linear combination of eigenvectors of $A$.

\begin{remark}
  $1_n$ is an eigenvector of $T$ with eigenvalue $\lambda = 1$, and every
  vector $x \in \R^n$ such that $\sum_i x_i = 0$ is an eigenvector of $T$ with
  $\lambda = -\frac{1}{n-1}$.
\end{remark}
\begin{proof}
  Multiplying a vector by the all-ones matrix takes the sum of that vector's
  values for every index in the result, so $J \cdot 1_n = n \cdot 1_n$.
  Similarly $Jx = 0_n$, for any $x$ s.t. $\sum_i x_i = 0$. Thus,

  \begin{align*}
    T\cdot 1_n &= \frac{1}{n-1}(J-I) \cdot 1_n \\
               &= \frac{1}{n-1}(n\cdot 1_n - 1_n) \\
               &= 1_n
  \end{align*}

  and

  \begin{align*}
    Tx &= \frac{1}{n-1}(J-I)x \\
       &= \frac{1}{n-1}(0_n - x) \\
       &= -\frac{1}{n-1}x
  \end{align*}
\end{proof}


Next, we will show how to write any $e_u$ as a linear combination of
eigenvectors of $T$. We define $\alpha_u$ by $\alpha_{u,u} := n-1$ and
$\alpha_{u,j} = -1, u\neq j$. The entries of $\alpha_u$ sum to $0$, so
$T\alpha_u=-\frac{1}{n-1}\alpha_u$. We can write
$e_u = \frac{1}{n}(1_n + \alpha_u)$.

\begin{corollary}
  The eigenvectors of $J-I$ span $\R^n$.
\end{corollary}
\begin{proof}
  Since all standard basis elements $e_j$ of $\R^n$ can be expressed as linear
  combinations of eigenvectors for $J-I$, the eigenvectors of $J-I$ span $\R^n$.
\end{proof}

\begin{theorem}
  Let $K_n$ be the complete graph with nodes labelled $1,...,n$. Then for any
  two distinct nodes $u$ and $v$, $\dsd(u,v) = \frac{2(n-1)}{n}$.
\end{theorem}
\begin{proof}
  We will use $\alpha_u$ as defined above. DSD is given by

\begin{align*}
  \dsd(u,v) &= \sum_{k = 0}^{\infty}{||T^ke_u - T^ke_v||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(1_n + \alpha_u - 1_n -
                \alpha_v)||_1}\\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(\alpha_u - \alpha_v)||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||(-\frac{1}{n-1})^k(\alpha_u -
                \alpha_v)||_1} \\
              &= \frac{||\alpha_u - \alpha_v||_1}{n}
                \sum_{k = 0}^{\infty}{(-\frac{1}{n-1})^k} \\
\end{align*}

Since $|-\frac{1}{n-1}| < 1$, the geometric series converges,
$\sum_{k=0}^{\infty}(-\frac{1}{n-1})^k = \frac{n-1}{n}$. When $u \neq v$, the
difference $\alpha_u - \alpha_v$ will have exactly two non-zero entries (since
they are identical at all indices but $u$ and $v$). These non-zero entries will
be $n$ at index $u$ and $-n$ at index $v$. Thus,
$||\alpha(u)-\alpha(v)||_1 = 2n$, and so

\begin{align*}
  \dsd(u,v) &= \frac{2n}{n}(\frac{n-1}{n}) \\
              &= \frac{2(n-1)}{n} \\
\end{align*}

\end{proof}

\subsection{Circulant Graphs}

Circulant graphs have well-understood spectra, so we can perform a similar
analysis to the complete case.

\begin{definition}
  We {\bf rotate} a row vector by moving each of its elements over one index to
  the right. The rightmost element wraps around and is put in the leftmost index
  in the result.
\end{definition}

\begin{example}
  The rotation of $\colvec{1 & 2 & 3 & 4}$ is $\colvec{4 & 1 & 2 & 3}$.
\end{example}

\begin{definition}
  A matrix is {\bf circulant} if each row is equal to the A graph is {\bf
    circulant} if it has a circulant adjacency matrix.
\end{definition}

\begin{example}
  The pentagon with this circulant adjacency matrix:
  \[
    \begin{bmatrix}
      0 & 1 & 0 & 0 & 1 \\
      1 & 0 & 1 & 0 & 0 \\
      0 & 1 & 0 & 1 & 0 \\
      0 & 0 & 1 & 0 & 1 \\
      1 & 0 & 0 & 1 & 0 \\
    \end{bmatrix}
  \]
\end{example}

\subsubsection*{Spectrum}

[The expressions for eigenvectors/eigenvalues are sourced from Wikipedia --JP]

All circulant matrices have the same eigenvectors given by

\[x_j = \colvec{1 \\ z^j \\ z^{2j} \\ ... \\ z^{(n-1)j}}\]

where $j=1,2,...,n$ and $z=\exp(\frac{2 \pi i}{n})$ is an $n^{th}$ root of
unity. Note that the $n^{th}$ eigenvector is $1_n$, and that multiplying $x_j$
by $z^{n-j}$ rotates it, as $1 = z^0 = z^n$. We can write our basis vectors $e_u$ as
linear combinations of elements in $x_j$ like so:

\[e_u = \frac{1}{n}(1_n + \sum^{n-1}_{j=1} z^{(u-1)(n-j)} x_j)\]

This depends on the fact that the sum of all $n^{th}$ roots of unity is 0, so
$1 + \sum_{j=1}^{n-1}z^j = 0$. Therefore, we get $ne_1$ by summing over all
eigenvectors, since every index except the first will vanish. We can get the
other basis vectors by repeatedly rotating all the eigenvectors except $1_n$,
which is where the $z^{(u-1)(n-k)}$ comes from in the form above.

The corresponding eigenvalues $\lambda_j$ depend on the matrix. Let
$r = \colvec{r_1 & r_2 & ... & r_n}$ be the first row of the matrix. Then, the
$j^{th}$ eigenvalue is given by

\[\lambda_j = \sum_{k}r_kz^{(k-1)j} \]


\subsubsection*{Cycles}

Cycles are the most obvious example of circulant graphs. The eigenvalues of a
cycle are given by $\lambda_j = z^{j-1} + z^{j+1}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
