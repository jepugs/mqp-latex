In this chapter, we provide background about metrics on graph vertices, with a particular focus on
the diffusion state distance (DSD). The three prediction algorithms compared in Chapters
\ref{chap:simulations} and \ref{chap:real_world} are identical except for the choice of metric, as a
key part of our project is understanding how that choice affects label prediction performance. For
details about how our classification algorithm uses metrics to pick labels, see Chapter
\ref{chap:prediction}.


\section{Definition and Shortest-Path Distance}

A metric is a nonnegative real-valued function that generalizes Euclidean distance between points in
$\mathbb{R}^n$ to general sets. In our case, the set is the vertices of the graph, and in the context of the
label prediction problem, we use metrics to determine similiarity of two vertices in the graph.

\begin{definition}
  A \textbf{metric} on a set $S$ is a function $f : S \times S \to \R$
  satisfying

  \begin{enumerate}
  \item $f(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $f(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $f(u,v) = f(v,u) ~\forall u,v \in G$, and
  \item $f(u,v) \leq f(u,w) + f(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}
\end{definition}

The standard metric on graphs is the shortest-path distance (definition \ref{def:walk_path}), whose
definition we reiterate here for convenience. The \textbf{shortest-path distance} between two
vertices $u,v \in G$, denoted $\spd(u,v)$, is the minimum number of edges in a path from $u$ to $v$.

\begin{proposition}
  Shortest-path distance is a metric (on the vertices of connected graphs).
\end{proposition}

\begin{proof}
  From the definition of shortest-path distance, it is clear that metric conditions (1), (2),
  and (3) are satisfied, since we're dealing with unweighted, undirected graphs.

  To prove (4), let any three vertices in a connected graph $u,v,w$ be given. Define $a=\spd(u,w)$
  and $b=\spd(v,w)$. Then there exists a path from $u$ to $w$ of length $a$ and from $w$ to $v$ of
  length $b$, and concatenating these paths gives a path from $u$ to $v$ of length $a+b$. Thus, by
  definition of shortest-path distance, $\spd(u,v) \leq a + b = \spd(u,w) + \spd(v,w)$ for any three
  vertices $u,v,w$, which is more than sufficient to show (4).
\end{proof}

\section{Resistance Distance}

\begin{definition}[from ~\cite{weissteinRD}]
  Let $G$ be a connected graph. Imagine that $G$ is an electrical network where every edge is a
  1-ohm resistor. The resistance distance between $u,v \in G$, denoted $\rd(u,v)$, is the reciprocal
  of the resistance between $u$ and $v$ as calculated by Kirchoff's laws, with the unit (ohms)
  dropped.

  Formally, we can define it as follows. Let $L$ be the graph laplacian of $G$ and define $\Gamma$
  as

  \[ \Gamma = L + \frac{\mathbf{1}}{n} \]

  where $\mathbf{1}$ is the matrix consisting of all $1$s. Then,

  \[ \rd(u,v) = (\Gamma)^{-1}_{ii} + (\Gamma)^{-1}_{jj} - 2(\Gamma)^{-1}_{ij}\]
\end{definition}

\begin{theorem}[Page 89, Theorem B of ~\cite{KleinRandic1993}]
  $\rd$ is a metric (on the vertices of an arbitrary connected graph)
\end{theorem}

Resistance distance is another example of a metric on graphs, and it is dramatically different from
shortest-path distance. One of its key characteristics is that as we add more paths connecting two
vertices, the distance between them decreases ~\cite{KleinRandic1993}. Because resistance distance
gives consideration to the entire graph (as opposed to just the shortest path), we expect it to be
more stable to noise in the form of randomly added or deleted edges than shortest path.


\section{Diffusion State Distance}

The diffusion state distance (DSD) is a metric on graphs which determines distance between two
vertices based upon the convergence behavior of random walks starting from each vertex. Intuitively,
it provides a way to measure vertex distance that is sensitive to the local structure of the graph,
and appears to be well suited to detecting community structures in graphs.

In the original paper, Cao, Zhang, Park, Daniels, Crovella, Cowen, and Hescott define DSD in terms
of a vector-valued function $\mathrm{He}^{k}(u)$, which computes the expected number of times a
length-$k$ random walk originating at a vertex $u$ will visit each other vertex in the graph. They
compute DSD by taking the $l_1$-norm of the difference between two such vectors in the limit as
$k \to \infty$. We present an alternative definition which is easier to manipulate with the usual
machinery of linear algebra. We begin by recalling some definitions.

\begin{definition}
  A \textbf{random walk} of a graph is a walk (see definition \ref{def:walk_path}) originating at
  some vertex in which each edge is picked uniformly randomly from all the edges adjoined to the
  vertex preceding it.
\end{definition}

\begin{definition}[Page 302 of \cite{Bollobas1998} ]
  A (discrete-time) \textbf{markov chain} on a finite set of states $V$ is a sequence of random
  variables $X_0,X_1,....$ taking values $x_o,x_1,... \in V$ such that for a given $t$, the
  probability of each outcome of $X_{t+1}$ depends only on $x_t$ (the outcome before it).

  Let $u,v \in V$. Then the conditional probability $P(X_{t+1}=v|X_t=u)$ is called the
  \textbf{transition probability} from $u$ to $v$.
  
  The \textbf{transition matrix} of a markov chain is the matrix $T$ given by

  \[
    T_{ij} = \text{The probability of transitioning from state $j$ to state $i$}
  \]
\end{definition}

Random walks may be modeled as markov chains where the states are the vertices in the graph. The
random walk transition matrix of a graph $G$ can be written $T = AD^{-1}$, where $A$ is the
adjacency and $D$ is the degree matrix, (definitions \ref{def:adj_mat} and \ref{def:deg_mat}). For
our purposes, we assume that vertices are labeled by the positive integers $1,2,...,n$ and that the
rows and columns of $G$ correspond to this ordering.

\begin{proposition}[Properties of $T$]
  Let $G$ be a graph and $T$ the transition matrix for the corresponding random walk markov chain.
  The following is true:
  \begin{enumerate}
  \item $T$ is irreducible
  \item every eigenvalue of $T$ has magnitude $\leq 1$
  \end{enumerate}
\end{proposition}

\begin{proof}
  To show (1), we first recall from proposition \ref{prop:adj} that the adjacency matrix is
  irreducible. From the definition $T = AD^{-1}$, we can see that $T$ has zeros in the same set of
  indices as $A$, and so must be irreducible by analogous argument.

  (2) follows from (1) and application of the Perron-Frobenius Theorem (Theorem
  \ref{thm:perron_frobenius}).
\end{proof}

Based on how we've defined the transition matrix, it can be thought of as an operator which takes a
marginal distribution of states in the markov chain, and outputs the marginal distribution after one
step. So, the marginal distribution of vertices in the $k\nth$ step of a random walk originating
from vertex $u$ is given by $T^ke_u$, where $e_u$ is the $u\nth$ standard basis vector in $\R^n$,
$e_{u,u} := 1$ and $e_{u,j} := 0, u \neq j$ for $u=1,...,n$.

Finally, we can define DSD.

\begin{definition}
  Let $G$ be a graph, $u,v \in G$. The \textbf{diffusion state distance (DSD)}, denoted $\dsd(u,v)$,
  is defined by
  \[
    \dsd(u,v) = ||\sum_{k =0}^{\infty}{T^ke_u - T^ke_v}||_1
  \]
  where $T$ is the transition matrix of $G$'s random walk markov chain and $e_i$ is th $i\nth$
  standard basis vector in $\mathbb{R}^n$.
\end{definition}

\begin{theorem}[Page 7 of \cite{10.1371/journal.pone.0076339}]
  DSD is a metric.
\end{theorem}

We defer to Cao et. al for proof of this.

As compared to shortest-path distance, we would expect DSD to be more resilient to random noise in
the form of randomly added and deleted edges. This is because DSD considers the structure of the
entire graph instead of just the shortest path. Resistance distance is also computed using the whole
graph, so we would expect that to be similarly resilient. However, DSD appears to be measuring the
similarity of two vertices' neighborhoods by comparing random walk convergence behavior, whereas
resistance distance is measuring electrical current capacity between two vertices. In the context of
most label prediction problems, where labels correspond to communities, this suggests intuitively
that DSD superior.


\section{DSD on the Complete Graph}

In this section, we work through the problem of computing DSD on the complete graph. This is not a
particularly illuminating result, but we hope it will help to familiarize the reader with the DSD.

Consider the complete graph $K_n$. In this case, $D = (n-1)I$ and $A = (J - I)$, so our transition
matrix $T$ is given by $\frac{1}{n-1}(J-I)$. In order to compute $T^ke_u$ in the limit, we will
represent $e_u$ as a linear combination of eigenvectors of $A$.

\begin{proposition}
  The all-ones vector $1_n$ is an eigenvector of $T$ with eigenvalue $\lambda = 1$, and every
  vector $x \in \R^n$ such that $\sum_i x_i = 0$ is an eigenvector of $T$ with
  $\lambda = -\frac{1}{n-1}$.
\end{proposition}
\begin{proof}
  Multiplying a vector by the all-ones matrix takes the sum of that vector's
  values for every index in the result, so $J \cdot 1_n = n \cdot 1_n$.
  Similarly $Jx = 0_n$, for any $x$ s.t. $\sum_i x_i = 0$. Thus,

  \begin{align*}
    T\cdot 1_n &= \frac{1}{n-1}(J-I) \cdot 1_n \\
               &= \frac{1}{n-1}(n\cdot 1_n - 1_n) \\
               &= 1_n
  \end{align*}

  and

  \begin{align*}
    Tx &= \frac{1}{n-1}(J-I)x \\
       &= \frac{1}{n-1}(0_n - x) \\
       &= -\frac{1}{n-1}x
  \end{align*}
\end{proof}


Next, we will show how to write any $e_u$ as a linear combination of
eigenvectors of $T$. We define $\alpha_u$ by $\alpha_{u,u} := n-1$ and
$\alpha_{u,j} = -1, u\neq j$. The entries of $\alpha_u$ sum to $0$, so
$T\alpha_u=-\frac{1}{n-1}\alpha_u$. We can write
$e_u = \frac{1}{n}(1_n + \alpha_u)$.

\begin{corollary}
  The eigenvectors of $J-I$ span $\mathbb{R}^n$.
\end{corollary}
\begin{proof}
  Since all standard basis elements $e_j$ of $\mathbb{R}^n$ can be expressed as linear
  combinations of eigenvectors for $J-I$, the eigenvectors of $J-I$ span $\mathbb{R}^n$.
\end{proof}

\begin{theorem}
  Let $K_n$ be the complete graph with nodes labelled $1,...,n$. Then for any
  two distinct nodes $u$ and $v$, $\dsd(u,v) = \frac{2(n-1)}{n}$.
\end{theorem}
\begin{proof}
  We will use $\alpha_u$ as defined above. DSD is given by

\begin{align*}
  \dsd(u,v) &= \sum_{k = 0}^{\infty}{||T^ke_u - T^ke_v||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(1_n + \alpha_u - 1_n -
                \alpha_v)||_1}\\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(\alpha_u - \alpha_v)||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||(-\frac{1}{n-1})^k(\alpha_u -
                \alpha_v)||_1} \\
              &= \frac{||\alpha_u - \alpha_v||_1}{n}
                \sum_{k = 0}^{\infty}{(-\frac{1}{n-1})^k} \\
\end{align*}

Since $|-\frac{1}{n-1}| < 1$, the geometric series converges,
$\sum_{k=0}^{\infty}(-\frac{1}{n-1})^k = \frac{n-1}{n}$. When $u \neq v$, the
difference $\alpha_u - \alpha_v$ will have exactly two non-zero entries (since
they are identical at all indices but $u$ and $v$). These non-zero entries will
be $n$ at index $u$ and $-n$ at index $v$. Thus,
$||\alpha(u)-\alpha(v)||_1 = 2n$, and so

\begin{align*}
  \dsd(u,v) &= \frac{2n}{n}(\frac{n-1}{n}) \\
              &= \frac{2(n-1)}{n} \\
\end{align*}

\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
