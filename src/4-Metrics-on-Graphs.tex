\section{Definition of DSD}

\noindent In their original definition, Cao, Zhang, Park, Daniels, Crovella, Cowen, and Hescott define
DSD in terms of a vector-valued function $\mathrm{He}^{k}(u)$, which computes the expected number of
times a length-$k$ random walk originating at a vertex $u$ will visit each other vertex in the graph.
They compute DSD by taking the $l_1$-norm of the difference between two such vectors in the limit as
$k \to \infty$. We present an alternative definition which is more compact and easier to manipulate
with the usual machinery of linear algebra.

%% Fix this so D is D inverse (the degree matrix inverse)

% TODO: more rigorously define random walks and markov chains
% ask question about
Random walks can be modeled as Markov chains, where the current state is a
vertex $u$ in the graph, and the transition probability is $\frac{1}{\deg(u)}$
for the neighbors of $u$ and $0$ otherwise. The transition matrix can be written
$T = AD$, where $A$ is the adjacency matrix of the graph and $D$ is the unique
diagonal matrix such that every column of $T$ sums to 1 (i.e.
$D_{u,u}=\frac{1}{deg(u)}$, or equivalently, $D$ is the inverse of the degree
matrix). For our purposes, we assume that vertices are labeled by the positive
integers $1,2,...,n$ and that the rows and columns of the $G$ correspond to this
ordering.

The marginal distribution of vertices in the $k\nth$ step of a random walk
originating from vertex $u$ is given by $T^ke_u$, where $e_u$ is the initial
state vector, which is the $u\nth$ standard basis vector in $\R^n$,
$e_{u,u} := 1$ and $e_{u,j} := 0, u \neq j$ for $u=1,...,n$.

Thus, we can define the DSD, $\dsd(u,v)$, by

\[
  \dsd(u,v) = ||\sum_{k =0}^{\infty}{T^ke_u - T^ke_v}||_1
\]

\subsection{Proof that DSD is a Metric}

A metric is a nonnegative real-valued function that generalizes Euclidean
distance between points in $R^n$ to general sets. In our case, the set is the
vertices of the graph. Metrics allow us to define metric spaces which are
generally useful, but the context of our problem, we are mainly interested in
them because they allow us to partially order all vertices in a graph based on
their distance from a fixed vertex.

\begin{definition}
  A \textbf{metric} with respect to a set $S$ is a function $d : S x S \to \R$
  satisfying

  \begin{enumerate}
  \item $d(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $d(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $d(u,v) = \dsd(v,u) ~\forall u,v \in G$, and
  \item $d(u,v) \leq \dsd(u,w) + \dsd(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}\end{definition}

First, we will define a finite analog to the $\mathrm{He(u)}$ vectors of Cao et al. Because
$T$ is a transition matrix for a Markov chain, this means that
$\lim_{k \to \infty} T^k$ converges to a stationary distribution which we will
call $T^\infty$.

\begin{lemma}
  Let $T$ be a transition matrix for a markov chain and $T^\infty$ the
  corresponding stationary distribution. The matrix sum

  \[
    S = \sum_{k=0}^{\infty}(T^k - T^\infty)
  \]

  converges.
\end{lemma}
\begin{proof}
  \textbf{TODO} [I thought this was an easy proof, but now I'm not sure -- JP]
\end{proof}

\begin{remark}
  $S$ is symmetric.
\end{remark}
\begin{proof}
  Clearly, since $T$ is symmetric and commutes with itself, $T^k$ is symmetric
  for all $k$. This implies that $T^\infty$ is also symmetric, as is the
  difference $T^k - T^\infty$ for all $k$. Thus, every term in the sum is
  symmetric, completing the proof.
\end{proof}

We will denote row vectors of matrices with a single subscript, i.e. $S_u$ is
the $u\nth$ row of $S$ which is equal to the $u\nth$ column. The columns of $S$
are our finite analog for the $\mathrm{He}$ vectors. Now, note that $T^ke_u$ is
equal to $T^k_u$. This allows us to rewrite DSD as

\begin{align*}
  \dsd(u,v) &= ||\sum_{k=0}^{\infty}((T^k)_u - (T^k)_v)||_1 \\
              &= ||\sum_{k=0}^{\infty}((T^k-T^\infty)_u - (T^k-T^\infty)_v)||_1 \\
              &= ||S_u - S_v||_1
\end{align*}

\begin{theorem}
  DSD is a metric.
\end{theorem}
\begin{proof}
  We must show:

  \begin{enumerate}
  \item $\dsd(u,v)\geq 0 ~ \forall u,v \in G$,
  \item $\dsd(u,v) = 0 \iff u = v$ (identity of indiscernables),
  \item $\dsd(u,v) = \dsd(v,u) ~\forall u,v \in G$, and
  \item $\dsd(u,v) \leq \dsd(u,w) + \dsd(v,w) ~\forall u,v,w \in G$
    (triangle inequality).
  \end{enumerate}

  Let $G = (V,E)$ be an arbitrary graph of order $n$ and define $T$, $T^\infty$,
  and $S$ as above. We define a map $f : V \to \R^n$ by $f(u) = S_u$. DSD is
  equivalent to taking the $l_1$-norm of differences between these points. Since
  $l_1$ distance is a metric in $\R^n$, we have shown (1), (3), and (4).

  Because $T$ is invertible, no two rows or columns of $T$ are equal, which also
  applies to the powers of $T$. Consequently, $S$ is also invertible. Thus,
  $f$ is injective, proving (2).

  % The eigenvectors of $T$ span $\R^n$, which also means that no two
  % rows of $T$ are equal, as otherwise $\dim T$ would be less than $n$. We
  % rewrite every row/column $T_u$ as a linear combination
  % \[
  %   T_u = \sum_{i=1}^n \theta_i(u) x_i
  % \]

  % for some coefficient vector $\theta(u)$, where $x_i$ are eigenvectors of $T$.
  % Let $\lambda_i$ be the respective eigenvalues. Because all the rows in $T$ are
  % distinct, every coefficient vector $\theta(u)$ is distinct. Also, the
  % corresponding coefficients for powers of $T$ are
  % $T^k_u = \sum{\lambda_i^{k-1} \theta_i(u) x_i}$.

  % Let $k$ be arbitrary, and suppose $T^k_u = T^k_v$ for some $u \neq v$. Then
  % $\lambda_i^{k-1}\theta_i(u) = \lambda_i^{k-1}\theta_i(v)$ for all
  % $i = 1,...,n$. Clearly, this only holds for the case that
  % $\theta_i(u) = \theta_i(v)$ for all $i$, which is a contradiction because it
  % implies $T_v = T_u$. Thus, every row (and column) of $T^k$ is distinct for all
  % powers $k$, and this property extends to $S$, thus proving (2).

\end{proof}


\section{DSD on Special Graphs}

\subsection{Complete Graphs}
Consider the complete graph $K_n$. In this case, $D = \frac{1}{n-1}I$ and
$A = (J - I)$, so $T=\frac{1}{n-1}(J-I)$. In order to compute $T^ke_u$ in the
limit, we will represent $e_u$ as a linear combination of eigenvectors of $A$.

\begin{remark}
  $1_n$ is an eigenvector of $T$ with eigenvalue $\lambda = 1$, and every
  vector $x \in \R^n$ such that $\sum_i x_i = 0$ is an eigenvector of $T$ with
  $\lambda = -\frac{1}{n-1}$.
\end{remark}
\begin{proof}
  Multiplying a vector by the all-ones matrix takes the sum of that vector's
  values for every index in the result, so $J \cdot 1_n = n \cdot 1_n$.
  Similarly $Jx = 0_n$, for any $x$ s.t. $\sum_i x_i = 0$. Thus,

  \begin{align*}
    T\cdot 1_n &= \frac{1}{n-1}(J-I) \cdot 1_n \\
               &= \frac{1}{n-1}(n\cdot 1_n - 1_n) \\
               &= 1_n
  \end{align*}

  and

  \begin{align*}
    Tx &= \frac{1}{n-1}(J-I)x \\
       &= \frac{1}{n-1}(0_n - x) \\
       &= -\frac{1}{n-1}x
  \end{align*}
\end{proof}


Next, we will show how to write any $e_u$ as a linear combination of
eigenvectors of $T$. We define $\alpha_u$ by $\alpha_{u,u} := n-1$ and
$\alpha_{u,j} = -1, u\neq j$. The entries of $\alpha_u$ sum to $0$, so
$T\alpha_u=-\frac{1}{n-1}\alpha_u$. We can write
$e_u = \frac{1}{n}(1_n + \alpha_u)$.

\begin{corollary}
  The eigenvectors of $J-I$ span $\R^n$.
\end{corollary}
\begin{proof}
  Since all standard basis elements $e_j$ of $\R^n$ can be expressed as linear
  combinations of eigenvectors for $J-I$, the eigenvectors of $J-I$ span $\R^n$.
\end{proof}

\begin{theorem}
  Let $K_n$ be the complete graph with nodes labelled $1,...,n$. Then for any
  two distinct nodes $u$ and $v$, $\dsd(u,v) = \frac{2(n-1)}{n}$.
\end{theorem}
\begin{proof}
  We will use $\alpha_u$ as defined above. DSD is given by

\begin{align*}
  \dsd(u,v) &= \sum_{k = 0}^{\infty}{||T^ke_u - T^ke_v||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(1_n + \alpha_u - 1_n -
                \alpha_v)||_1}\\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||T^k(\alpha_u - \alpha_v)||_1} \\
              &= \frac{1}{n}\sum_{k = 0}^{\infty}{||(-\frac{1}{n-1})^k(\alpha_u -
                \alpha_v)||_1} \\
              &= \frac{||\alpha_u - \alpha_v||_1}{n}
                \sum_{k = 0}^{\infty}{(-\frac{1}{n-1})^k} \\
\end{align*}

Since $|-\frac{1}{n-1}| < 1$, the geometric series converges,
$\sum_{k=0}^{\infty}(-\frac{1}{n-1})^k = \frac{n-1}{n}$. When $u \neq v$, the
difference $\alpha_u - \alpha_v$ will have exactly two non-zero entries (since
they are identical at all indices but $u$ and $v$). These non-zero entries will
be $n$ at index $u$ and $-n$ at index $v$. Thus,
$||\alpha(u)-\alpha(v)||_1 = 2n$, and so

\begin{align*}
  \dsd(u,v) &= \frac{2n}{n}(\frac{n-1}{n}) \\
              &= \frac{2(n-1)}{n} \\
\end{align*}

\end{proof}

\subsection{Circulant Graphs}

Circulant graphs have well-understood spectra, so we can perform a similar
analysis to the complete case.

\begin{definition}
  We {\bf rotate} a row vector by moving each of its elements over one index to
  the right. The rightmost element wraps around and is put in the leftmost index
  in the result.
\end{definition}

\begin{example}
  The rotation of $\colvec{1 & 2 & 3 & 4}$ is $\colvec{4 & 1 & 2 & 3}$.
\end{example}

\begin{definition}
  A matrix is {\bf circulant} if each row is equal to the A graph is {\bf
    circulant} if it has a circulant adjacency matrix.
\end{definition}

\begin{example}
  The pentagon with this circulant adjacency matrix:
  \[
    \begin{bmatrix}
      0 & 1 & 0 & 0 & 1 \\
      1 & 0 & 1 & 0 & 0 \\
      0 & 1 & 0 & 1 & 0 \\
      0 & 0 & 1 & 0 & 1 \\
      1 & 0 & 0 & 1 & 0 \\
    \end{bmatrix}
  \]
\end{example}

\subsubsection*{Spectrum}

[The expressions for eigenvectors/eigenvalues are sourced from Wikipedia --JP]

All circulant matrices have the same eigenvectors given by

\[x_j = \colvec{1 \\ z^j \\ z^{2j} \\ ... \\ z^{(n-1)j}}\]

where $j=1,2,...,n$ and $z=\exp(\frac{2 \pi i}{n})$ is an $n^{th}$ root of
unity. Note that the $n^{th}$ eigenvector is $1_n$, and that multiplying $x_j$
by $z^{n-j}$ rotates it, as $1 = z^0 = z^n$. We can write our basis vectors $e_u$ as
linear combinations of elements in $x_j$ like so:

\[e_u = \frac{1}{n}(1_n + \sum^{n-1}_{j=1} z^{(u-1)(n-j)} x_j)\]

This depends on the fact that the sum of all $n^{th}$ roots of unity is 0, so
$1 + \sum_{j=1}^{n-1}z^j = 0$. Therefore, we get $ne_1$ by summing over all
eigenvectors, since every index except the first will vanish. We can get the
other basis vectors by repeatedly rotating all the eigenvectors except $1_n$,
which is where the $z^{(u-1)(n-k)}$ comes from in the form above.

The corresponding eigenvalues $\lambda_j$ depend on the matrix. Let
$r = \colvec{r_1 & r_2 & ... & r_n}$ be the first row of the matrix. Then, the
$j^{th}$ eigenvalue is given by

\[\lambda_j = \sum_{k}r_kz^{(k-1)j} \]


\subsubsection*{Cycles}

Cycles are the most obvious example of circulant graphs. The eigenvalues of a
cycle are given by $\lambda_j = z^{j-1} + z^{j+1}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Main"
%%% End:
